# 前端项目容器化部署问题

##### 这只是记录一个问题，而非任何解决方案

公司最近想要将各种服务迁移到k8s中，后端服务迁移比较顺利，前端迁移的时候遇到了一个问题：

> 前端的静态资源版本控制怎么和容器化方案结合起来？

现在前端使用打包工具生成的静态文件是带有md5戳的，md5值由内容产生，而index.html或者懒加载配置相当于一个清单，md5戳使新文件区分于旧版本从而快速更新至前端，在文件不发生变动时又可以充分利用缓存。这是前端工程的标准解决方案，之前在虚拟机中部署时没什么问题，服务器把打包后的文件rsync到代理目录中去就行了，但是要把静态资源容器化的时候就有问题了，因为容器化的镜像不含有历史文件，所以容器化部署的时候正在使用旧文件的用户很有可能造成页面崩溃。这个问题想过以下几个方法：

1. 使用CDN回源，将CDN回源host设定为部署service地址。这个方法可以将用户访问过的历史版本保存在CDN中，但是不能完全解决问题，因为无法确切保证所有文件均被用户访问过，仍有小概率的崩溃可能性。
2. 主动上传CDN，将所有文件上传至CDN可以解决上一条中的问题，这样pod中只需要保留nginx和index.html即可，但是实际执行的时候，上传时机是一个问题。这里也有几个选择：
    *. 人工手动上传，这意味着完全抛弃CI
    *. 放在CI中，这意味着要在CI中添加对应CDN的token或者通过其他外部服务获取
    *. 放在部署的pod中，这意味着上线完毕后不能立即更新index.html（因为静态资源还没上传完），但是旧的index.html所在的pod已经被杀掉了，需要一些复杂的控制逻辑
可见哪一种都不算特别理想，硬选一个的话第二种勉强可以使用
3. 一种理想化的方法，就是把旧的pod持久化用户通过sessionAffinity: ClientIP特性访问旧pod，如果用户刷新的话优先反代到新的pod中去，待到旧pod无人或者可接受的人数访问时自动杀掉。这种机制k8s本身不支持，修改门槛太高了。
4. 另外一种方法，记录多个版本，在生成镜像过程中from第一个版本使用docker构建第二第三个版本，以此类推，相当于把历史文件放到了docker镜像的不同layers中去，这个也可以解决问题，但是问题时image会变得很大，影响发布效率，理想考虑的话，可以使用git diff只将发生变化的文件重新计算md5放进docker中，而不发生变化的文件根据docker layer的访问策略还是可以访问到，不过代码工作量和逻辑也比较复杂。